# packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
import umap
from sklearn.cluster import KMeans
from scipy.stats import zscore

# load the file 
df = pd.read_excel("tpg2plantgenome2015040025-sup-0001.xlsx")

#Remove Outliers
z_scores = np.abs(df[['Z1', 'Z2', 'Z3', 'Z4']].apply(zscore))# Z-scores
df_filtered = df[(z_scores < 3).all(axis=1)]# (just for now 3) ## Set threshold
print(f"Removed {len(df) - len(df_filtered)} outliers.")
# Removed 259 outliers.

# Remove low-expressed genes
df_filtered = df_filtered[df_filtered[['Z1', 'Z2', 'Z3', 'Z4']].max(axis=1) > 5]# set a threshold (e.g., keep genes with max expression >5)
print(f"Removed {len(df) - len(df_filtered)} low-expressed genes.")
# Removed 325 low-expressed genes.

#Select highly variable genes for analysis
df['Variance'] = df[['Z1', 'Z2', 'Z3', 'Z4']].var(axis=1)
threshold = df_filtered['Variance'].quantile(0.75)
df_filtered = df_filtered[df_filtered['Variance'] > threshold]
print(f"Selected {len(df_filtered)} highly variable genes.")
# Selected 2337 highly variable genes.

# Standardize
X = df_filtered[['Z1', 'Z2', 'Z3', 'Z4']]  # Numerical features
y = df_filtered['Cluster']  # Labels (K1-K6)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#plot the data before standardization
sns.histplot(X.sum(axis=1), discrete=False, legend=False)

# log2 tranformation
df_filtered[['Z1', 'Z2', 'Z3', 'Z4']] = np.log2(1 + df_filtered[['Z1', 'Z2', 'Z3', 'Z4']])

# Then extract features again
X = df_filtered[['Z1', 'Z2', 'Z3', 'Z4']]
X_scaled = scaler.fit_transform(X)

#plot after log-transformation
sns.histplot(X_scaled.sum(axis=1), kde=True)

#split into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

# Initialize RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit to the training data
rf.fit(X_train, y_train)

# Predict on the test data
y_pred = rf.predict(X_test)

# Print the accuracy of the prediction on the test data
print("Accuracy:", accuracy_score(y_test, y_pred))
# Print the classification report
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Plot importance of each feature
feature_importance = pd.Series(rf.feature_importances_, index=['Z1', 'Z2', 'Z3', 'Z4'])
feature_importance.sort_values().plot(kind='barh', title="Feature Importance in RF")
plt.show()

# Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True)
plt.show()

# UMAP 
umap_model = umap.UMAP(n_components=2, random_state=42)
X_umap = umap_model.fit_transform(X_scaled)

# K-Means Clustering
kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_umap)

# Visualize the clustering
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=clusters, palette="viridis")
plt.title("UMAP Projection with K-Means Clusters")
plt.show()
